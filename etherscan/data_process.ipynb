{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "'''\n",
    "不用看，测试文件\n",
    "data_process.ipynb和data_fetch.ipynb是获取数据以及处理的一个流程，和cal_pnL.ipynb，fetch_process_function.py是同样的作用，后者是将前者的代码整理成一个py文件，方便调用，更加简洁\n",
    "'''\n",
    "OUTPUT_FILE = 'C:/Users/YuweiCao/Documents/GitHub/Project/Project/etherscan/result'\n",
    "api_key = \"VQAIR728IM4Z8RZKPYBR4ESM5I3WBZK2C1\" # my free API key, you can get one at https://etherscan.io/myapikey\n",
    "base_url = \"https://api.etherscan.io/v2/api\" # We're using the v2 API 2024/12/12\n",
    "ADDRESS = \"0x5be9a4959308A0D0c7bC0870E319314d8D957dBB\" # Address of the contract we want to get the source code of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_three_records(grouped_df, data):\n",
    "    matched_hashes = []\n",
    "\n",
    "    for tx_hash, group in grouped_df:\n",
    "        if len(group) == 3:\n",
    "            print(f\"⚠️ High Alert: Transaction Hash {tx_hash} contains 3 records:\")\n",
    "            print(group)\n",
    "            matched_hashes.append(tx_hash)\n",
    "\n",
    "    # i want to delete the abnormal data\n",
    "    remaining_data = data[~data['hash'].isin(matched_hashes)].reset_index(drop=True)\n",
    "    return remaining_data\n",
    "\n",
    "\n",
    "def format_number(value):\n",
    "    if value < 1_000:\n",
    "        return f\"{int(value)}\"\n",
    "    elif value < 1_000_000:\n",
    "        return f\"{value / 1_000:.1f}K\"\n",
    "    elif value < 1_000_000_000:\n",
    "        return f\"{value / 1_000_000:.1f}M\"\n",
    "    else:\n",
    "        return f\"{value / 1_000_000_000:.1f}B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global BASE_TOKENS\n",
    "BASE_TOKENS = {\"USDT\", \"USDC\", \"USDE\"}\n",
    "ADDRESS = ADDRESS.lower()\n",
    "\n",
    "csv_file = 'C:/Users/YuweiCao/Documents/GitHub/Project/Project/etherscan/result/erc20_transfers.csv'\n",
    "transaction_data = pd.read_csv(csv_file)\n",
    "# print(transaction_data.head())\n",
    "transaction_data['dateTime'] = pd.to_datetime(transaction_data['dateTime'])\n",
    "transaction_data = transaction_data.sort_values(by=['dateTime', 'hash']).reset_index(drop=True)\n",
    "# make sure the date data is correct\n",
    "\n",
    "columns_to_keep = [\n",
    "    'dateTime', 'blockNumber', 'timeStamp', 'hash', 'from', 'to',\n",
    "    'value', 'tokenName', 'tokenSymbol'\n",
    "]\n",
    "\n",
    "duplicate_hashes = transaction_data[transaction_data.duplicated(subset=['hash'], keep=False)]\n",
    "transaction_data_1 = transaction_data[~transaction_data['hash'].isin(duplicate_hashes['hash'])].reset_index(drop=True)\n",
    "filtered_transaction_data = transaction_data_1[columns_to_keep]\n",
    "output_file = 'filtered_transaction_data.csv'\n",
    "filtered_transaction_data.to_csv(output_file, index=False)\n",
    "# just a check, if there are abnoraml data, print it and delete from original data\n",
    "three_record_hashes = highlight_three_records(duplicate_hashes.groupby('hash'), transaction_data)\n",
    "\n",
    "duplicate_hashes = duplicate_hashes[~duplicate_hashes['hash'].isin(three_record_hashes)]\n",
    "\n",
    "output_records = []\n",
    "\n",
    "for hash_val, group in duplicate_hashes.groupby('hash'):\n",
    "    base_tokens = group[group['tokenSymbol'].isin(BASE_TOKENS)]\n",
    "    other_tokens = group[~group['tokenSymbol'].isin(BASE_TOKENS)]\n",
    "    \n",
    "    if not base_tokens.empty and not other_tokens.empty:\n",
    "        if base_tokens['to'].iloc[0] == ADDRESS:\n",
    "            transaction_type = \"SELL\"\n",
    "        else:\n",
    "            transaction_type = \"BUY\"\n",
    "\n",
    "        base_token_info = f\"{base_tokens['value'].sum()} {base_tokens['tokenSymbol'].iloc[0]}\"\n",
    "        other_token_info = f\"{other_tokens['value'].sum()} {other_tokens['tokenSymbol'].iloc[0]}\"\n",
    "\n",
    "        record = f\"{group['timeStamp'].iloc[0]} W {transaction_type} {other_token_info} of {base_token_info} (at {group['dateTime'].iloc[0]})\"\n",
    "        output_records.append({\n",
    "            \"formatted_record\": record,\n",
    "            \"timeStamp\": group['timeStamp'].iloc[0],\n",
    "            \"dateTime\": group['dateTime'].iloc[0]\n",
    "        })\n",
    "    elif base_tokens.empty and not other_tokens.empty:\n",
    "        # 如果 tokenSymbol 都不在 BASETOKENS 中，额外输出一下\n",
    "        record = f\"{group['timeStamp'].iloc[0]} Same Hash {hash_val}: Tokens not in BASE_TOKENS: {', '.join(group['tokenSymbol'].unique())} (at {group['dateTime'].iloc[0]})\"\n",
    "        output_records.append({\n",
    "            \"formatted_record\": record,\n",
    "            \"timeStamp\": group['timeStamp'].iloc[0],\n",
    "            \"dateTime\": group['dateTime'].iloc[0]\n",
    "        })\n",
    "        \n",
    "output_df = pd.DataFrame(output_records)\n",
    "if not output_df.empty:\n",
    "    output_df = output_df.sort_values(by='timeStamp').reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nFormatted Transactions:\")\n",
    "    for record in output_df['formatted_record']:\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5be9a4959308a0d0c7bc0870e319314d8d957dbb 0xe217e15b3c19cc0427f9492dc3bcfe8220afad10 0x9008d19f58aabd9ed0d60971565aa8510560ab41 0x5be9a4959308a0d0c7bc0870e319314d8d957dbb\n",
      "0x9008d19f58aabd9ed0d60971565aa8510560ab41 0x5be9a4959308a0d0c7bc0870e319314d8d957dbb 0x5be9a4959308a0d0c7bc0870e319314d8d957dbb 0x78ff9211317620de95602c9cbed3ae803689e545\n",
      "1733940791 W 'BUY' 2187457492438 USDC of 10000000000000 OPTIMUS (at 2024-12-11 18:13:11)\n"
     ]
    }
   ],
   "source": [
    "matched_records = []\n",
    "matched_indices = []\n",
    "\n",
    "skip_next = False\n",
    "for i in range(len(transaction_data_1) - 1):\n",
    "    if skip_next:\n",
    "        skip_next = False\n",
    "        continue\n",
    "\n",
    "    current_row = transaction_data_1.iloc[i]\n",
    "    next_row = transaction_data_1.iloc[i + 1]\n",
    "\n",
    "    if ((current_row['to'] == ADDRESS and next_row['from'] == ADDRESS) or\n",
    "        (current_row['from'] == ADDRESS and next_row['to'] == ADDRESS)):\n",
    "        \n",
    "        if ((current_row['tokenSymbol'] in BASE_TOKENS or next_row['tokenSymbol'] in BASE_TOKENS) and\n",
    "        not (current_row['tokenSymbol'] in BASE_TOKENS and next_row['tokenSymbol'] in BASE_TOKENS)):\n",
    "            \n",
    "            if current_row['to'] == ADDRESS:\n",
    "                transaction_type = \"\\'SELL\\'\"\n",
    "                base_token = current_row\n",
    "                other_token = next_row\n",
    "            else:\n",
    "                transaction_type = \"\\'BUY\\'\"\n",
    "                base_token = next_row\n",
    "                other_token = current_row\n",
    "\n",
    "            base_token_info = f\"{base_token['value']} {base_token['tokenSymbol']}\"\n",
    "            other_token_info = f\"{other_token['value']} {other_token['tokenSymbol']}\"\n",
    "\n",
    "            record = (\n",
    "                f\"{current_row['timeStamp']} W {transaction_type} {other_token_info} of {base_token_info} \"\n",
    "                f\"(at {current_row['dateTime']})\"\n",
    "            )\n",
    "            \n",
    "            matched_records.append({\"formatted_record\": record, \"dateTime\": current_row['dateTime']})\n",
    "            matched_indices.extend([i, i + 1])\n",
    "\n",
    "            skip_next = True\n",
    "\n",
    "# delete the matched data for later processing\n",
    "transaction_data_2 = transaction_data_1.drop(index=matched_indices).reset_index(drop=True)\n",
    "\n",
    "matched_df = pd.DataFrame(matched_records)\n",
    "\n",
    "# combine the first two dataframes and sort by time\n",
    "combined_df = pd.concat([output_df, matched_df], ignore_index=True)\n",
    "\n",
    "if not combined_df.empty:\n",
    "    combined_df = combined_df.sort_values(by='dateTime').reset_index(drop=True)\n",
    "    for record in combined_df['formatted_record']:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1733794511 W single BUY 25541000000 USDT (at 2024-12-10 01:35:11)\n",
      "1733808299 W single BUY 682700000 USDT (at 2024-12-10 05:24:59)\n",
      "1733809571 W single BUY 103070000 USDT (at 2024-12-10 05:46:11)\n",
      "1733823263 W single BUY 100035268 USDC (at 2024-12-10 09:34:23)\n",
      "1733827811 W single BUY 188000000 USDT (at 2024-12-10 10:50:11)\n",
      "1733829875 W single BUY 990000000 USDT (at 2024-12-10 11:24:35)\n",
      "1733831435 W single BUY 239131209 USDT (at 2024-12-10 11:50:35)\n",
      "1733831567 W single SELL 5470000000000000 WETH (at 2024-12-10 11:52:47)\n",
      "1733837087 W single BUY 1043248700 USDT (at 2024-12-10 13:24:47)\n",
      "1733842199 W single BUY 4923000000 USDT (at 2024-12-10 14:49:59)\n",
      "1733852735 W single BUY 22082304898 USDT (at 2024-12-10 17:45:35)\n",
      "1733853203 W single BUY 340240000 USDT (at 2024-12-10 17:53:23)\n",
      "1733863355 W single BUY 122572044 USDC (at 2024-12-10 20:42:35)\n",
      "1733863535 W single BUY 183506266 USDT (at 2024-12-10 20:45:35)\n",
      "1733876159 W single BUY 100000000 USDC (at 2024-12-11 00:15:59)\n",
      "1733877095 W single SELL 769695778409200000000 WLFI (at 2024-12-11 00:31:35)\n",
      "1733879579 W single BUY 9999900000000 USDC (at 2024-12-11 01:12:59)\n",
      "1733886083 W single BUY 6000000000 USDT (at 2024-12-11 03:01:23)\n",
      "1733887859 W single BUY 975000000 USDT (at 2024-12-11 03:30:59)\n",
      "1733905319 W single BUY 3946510000 USDT (at 2024-12-11 08:21:59)\n",
      "1733911067 W single BUY 75000000 USDT (at 2024-12-11 09:57:47)\n",
      "1733911895 W single BUY 148490625 USDT (at 2024-12-11 10:11:35)\n",
      "1733915039 W single BUY 10004158509 USDT (at 2024-12-11 11:03:59)\n",
      "1733917115 W single BUY 130000000 USDT (at 2024-12-11 11:38:35)\n",
      "1733928227 W single BUY 5400000000 USDT (at 2024-12-11 14:43:47)\n",
      "1733928707 W single BUY 972510100 USDT (at 2024-12-11 14:51:47)\n",
      "1733929391 W single BUY 201440000 USDT (at 2024-12-11 15:03:11)\n",
      "1733934983 W single BUY 1419806887 USDT (at 2024-12-11 16:36:23)\n",
      "1733940743 W single SELL 625029131134 USDC (at 2024-12-11 18:12:23)\n",
      "1733940767 W single SELL 2187513376428 USDC (at 2024-12-11 18:12:47)\n",
      "1733940791 W 'BUY' 2187457492438 USDC of 10000000000000 OPTIMUS (at 2024-12-11 18:13:11)\n",
      "1733946035 W single SELL 3500000000000000000000 LMWR (at 2024-12-11 19:40:35)\n",
      "1733953895 W single BUY 1009692044 USDT (at 2024-12-11 21:51:35)\n",
      "1733960351 W single BUY 23731855 USDT (at 2024-12-11 23:39:11)\n"
     ]
    }
   ],
   "source": [
    "single_records = []\n",
    "\n",
    "for i, row in transaction_data_2.iterrows():\n",
    "    if row['tokenSymbol'] in BASE_TOKENS:\n",
    "        if row['from'] == ADDRESS:\n",
    "            transaction_type = \"single SELL\"\n",
    "        elif row['to'] == ADDRESS:\n",
    "            transaction_type = \"single BUY\"\n",
    "    else:\n",
    "        if row['from'] == ADDRESS:\n",
    "            transaction_type = \"single BUY\"\n",
    "        elif row['to'] == ADDRESS:\n",
    "            transaction_type = \"single SELL\"\n",
    "\n",
    "    record = f\"{row['timeStamp']} W {transaction_type} {row['value']} {row['tokenSymbol']} (at {row['dateTime']})\"\n",
    "    single_records.append({\n",
    "        \"formatted_record\": record,\n",
    "        \"dateTime\": row['dateTime'],\n",
    "        \"timeStamp\": row['timeStamp'],\n",
    "        \"hash\": row['hash']\n",
    "    })\n",
    "\n",
    "single_df = pd.DataFrame(single_records)\n",
    "\n",
    "final_combined_df = pd.concat([output_df, matched_df, single_df], ignore_index=True)\n",
    "if not final_combined_df.empty:\n",
    "    final_combined_df = final_combined_df.sort_values(by='dateTime').reset_index(drop=True)\n",
    "    for record in final_combined_df['formatted_record']:\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
